% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt,a4paper]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.
\usepackage[numbers, sort&compress]{natbib}
%\usepackage[square]{natbib}
%\usepackage{scicite}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{eqnarray}
% Use times if you have the font installed; otherwise, comment out the
% following line.

\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.
\geometry{a4paper,scale=0.8}
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 20cm 
\textheight 21cm
\footskip 1.0cm
\geometry{top=1.5cm}
%The next command sets up an environment for the abstract to your paper.
\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}
\renewcommand\refname{References and Notes}
\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Introduction to Machine Learning} 

%\author{Jiafeng Shou} 
\time 0
\begin{document} 
% Double-space the manuscript.

\baselineskip24pt
% Make the title.
\maketitle 
%\tableofcontents

\section{Lecture 1}
% \begin{eqnarray}
% norm({\rm {\bf W}_i(\theta_j)})=\sqrt{x_1^2+x_2^2+\ldots+x_n^2}\\
% {\rm {\bf \bar W}_i(\theta_j)} = \frac{\rm {\bf W}_i(\theta_j)}{norm({\rm {\bf W}_i(\theta_j)})}
% \end{eqnarray}
%$$\int_{2}^{3}x^2dx$$
\subsection*{linear function}
$$
y = f_{\bf W}({\bf X})=f({\bf X,W})={\bf W^{\rm T}X}
$$
\subsubsection*{linear classifier (perception model)}
$$
{\bf x}_i \cdot {\bf w}+b \geq 0
$$
$$
{\bf x}_i \cdot {\bf w}+b < 0
$$
\subsubsection*{linear regression in 1 dimension}
$$
y^i = {\bf W}^{\rm T}{\bf X}^i+\epsilon^i
$$
where $\epsilon$ is the noise(loss).\\
Loss function: sum of squared errors
$$
L({\bf W}) = \sum_{i=1}^{N}(\epsilon^i)
^2$$
$$
L(w_0,w_1) = \sum_{i=1}^{N}
$$
%\nabla
$$
\frac{\partial L(w_0,w_1)}{\partial w_0} = \sum_{i=1}^{N} \frac{\partial[y^i-(w_0x_0^i+w_1x_1^i)]^2}{\partial w_0} = -2\sum_{i=1}^{N}(y^i-(w_0x_0^i+w_1x_1^i))x_0^i = 0
$$
$$
\sum_{i=1}^{N}y^ix^i_0 = w_0\sum_{i=1}^{N}x_0^ix_0^i+w_1\sum_{i=1}^{N}x_1^ix_0^i
$$
as follow, the partial gradient of $w_1$ would be 
$$
\sum_{i=1}^{N}y^ix^i_1 = w_0\sum_{i=1}^{N}x_0^ix_1^i+w_1\sum_{i=1}^{N}x_1^ix_1^i
$$
Therefore
\begin{equation}
\left[
\begin{matrix}
\sum_{i=1}^{N}y^ix_0^i \\
\sum_{i=1}^{N}y^ix_1^i \\
\end{matrix}
\right]=
\left[
\begin{matrix}
\sum_{i=1}^{N}x^i_0x_0^i & \sum_{i=1}^{N}x_0^ix_1^i \\
\sum_{i=1}^{N}x_0^ix_1^i & \sum_{i=1}^{N}x_1^ix_1^i \\
\end{matrix}
\right]
\left[
\begin{matrix}
w_0 \\
w_1 \\
\end{matrix}
\right]
\end{equation}
Formally, it could conclude that
$$
{\bf X}^{\rm T}{\bf y} = {\bf X}^{\rm T}{\bf X}{\bf w}
$$
$$
{\bf w} = ({\bf X}^{\rm T}{\bf X})^{-1}{\bf X}^{\rm T}{\bf y}
$$
Here still need to add the trace version( more generalized version):
\begin{eqnarray*}
\bf \hat{Y} &=& \bf Xw \\
Loss &=& (\bf Y-Xw)^2\\
 &=& \bf (Y-Xw)^T(Y-Xw)\\
 &=& \bf Y^TY-w^TX^TY-Y^TXw+w^TX^TXw\\
Tr[\frac{\partial}{\partial{\bf w}} Loss] &=& \bf -X^TY-X^TY+X^TXw+X^TXw \\
&=& 0 \\
\bf w&=& (X^TX)^{-1}X^TY
\end{eqnarray*}
Due to the matrix derivatives:
\begin{eqnarray*}
\rm Tr[ABC]&=& \rm Tr[CAB]\\
\rm \frac{\partial}{\partial A}Tr[A^TB] &=& B\\
\rm \frac{\partial}{\partial A}Tr[A^TBAC] &=& BAC+B^TAC^T
\end{eqnarray*}
\subsubsection*{Least squares solution, vector form}
\begin{eqnarray*}
L({\bf w})&=&({\bf y}-{\bf X}{\bf w})^t({\bf y}-{\bf X}{\bf w})\\
1&=&2
\end{eqnarray*}

\subsubsection*{Why the gradient could be equal to 0}
Hessian matrix is a square matrix of second-order partial derivatives of scalar-valued function, or scalar field.
\begin{equation}
{\bf H}(f) = \left[
\begin{matrix}
\frac{\partial^2f}{\partial x_1^2} & \frac{\partial^2f}{\partial x_1 \partial x_2} & \ldots & \frac{\partial^2f}{\partial x_1 \partial x_n}\\
\frac{\partial^2f}{\partial x_2 \partial x_1} & \frac{\partial^2f}{\partial x_2^2} & \ldots & \frac{\partial^2f}{\partial x_2 \partial x_n}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{\partial^2f}{\partial x_n \partial x_1} & \frac{\partial^2f}{\partial x_n \partial x_2} & \ldots & \frac{\partial^2f}{\partial x_n^2}\\
\end{matrix}
\right]
\end{equation}
$$
{\bf H} = \left|
\begin{matrix}
\frac{\partial^2f}{\partial x^2} & \frac{\partial^2f}{\partial x \partial y}\\
 \frac{\partial^2f}{\partial y \partial x} & \frac{\partial^2f}{\partial x^2}\\
\end{matrix}
\right|
$$
In the 2 dimension, when ${\bf H}>0$: if $\frac{\partial^2f}{\partial x^2}>0$, then point$(x_0,y_0)$ is the local min point. If $\frac{\partial^2f}{\partial x^2}<0$, then point$(x_0,y_0)$ is the local max point.\\
when ${\bf H}<0$, then point$(x_0,y_0)$ is the  stationary point.\\
when ${\bf H}=0$, second order cannot decide the point property, then consider it in higher order Taylor's Expansion.\\
In the example,${\bf H} = 4(x_0^i)^2(x_1^i)^2-4(x_1^ix_0^i)(x_0^ix_1^i)=0$, and $\frac{\partial^2f}{\partial x^2}=4(x_0^i)^2(x_1^i)^2>0$. Therefore, it is a local min point for the loss function.
\\
In higher dimension space (multi-variables), ${\bf H}(f)$ should be a positive definite matrix($(\nabla {\bf x})^{\rm T}{\bf H}(f)\nabla {\bf x}\geq 0$ for any $\nabla {\bf x}$).\\
The more detail of Hessian matrix could look up Taylor expansion.
\subsubsection*{Generalized linear regression}
$$
L({\bf w}) = \sum_{i=1}^{N}(y^i-{\bf w}^{\rm T}\phi({\rm x}^i))^{\rm T}
$$
where $\phi({\bf x}^i)$ is a polynomial function for ${\bf x}^i$
\subsubsection*{normalization}
L2 norm(euclidean) norm:
$$
||{\bf w}||_2 = \sqrt{\sum_{d=1}^{D}w_d^2}=\sqrt{<{\bf w},{\bf w}>}
$$\\
L1 norm(manhattan) norm:
$$
||{\bf w}||_1 = \sum_{d=1}^{D}|w_d|
$$\\
Lp norm, $p>1$:
$$
||{\bf w}||_p =(\sum_{d=1}^{D}w^p_d)^{\frac{1}{p}} 
$$
\subsection*{Ridge regression: L2-regularized linear regression}
\begin{eqnarray*}
L({\bf w}) &=& {\bf \epsilon}^{\rm T}{\bf \epsilon}+\lambda{\bf w}^{\rm T}{\bf w}={\bf y}^{\rm T}{\bf y}-2{\bf y}^{\rm T}{\bf X}{\bf w}+{\bf w}^{\rm T}({\bf X}^{\rm T}{\bf X}+\lambda{\bf I}){\bf w} \\
\nabla L({\bf w}^*) &=& 0 \\
{\bf w}^* &=& ({\bf X}^{\rm T}{\bf X}+\lambda{\bf I})^{-1}{\bf X}^{\rm T}{\bf y}
\end{eqnarray*}
In some case, the matrix cannot be inversed, then add $\lambda {\bf I}$ to make it invertible..\\
invertible matrix
\subsection*{Lasso regression: L1-regularized linear regression}
%\nabla L({\bf w}^*) = 0
suitable for small sample, large dimension.\\
It could shrink some coefficient into 0, helpful for feature selection.\\
The optimization would be gradient descent, LARS, PGD.

\subsection*{Logistic regression}
sigmoid function:
$$
\sigma(x)=\frac{1}{1+\exp(-x)}
$$
Given training set:$\{({\bf x}^1,y^1),\ldots,({\bf x}^N,y^N)\},{\bf x} \in \mathbb{R}^D, y\in\{0,1\}$
The ML function would be:
\begin{eqnarray*}
p(y^1,\ldots,y^n|{\bf x}^1,\ldots,{\bf x}^N) &=& \prod_{i=1}^{N}P(y^i|{\bf x}^i)\\
&=& \prod_{i=1}^{N}\sigma({\bf w}^T{\bf x}^i)^{y^i}(1-\sigma({\bf w}^T{\bf x}^i))^{1-y^i}\\
logP({\bf y|X;w}) &=& \sum_{i=1}^{N}y^ilog\sigma({\bf w}^T{\bf x}^i)+(1-y^i)log(1-\sigma({\bf w}^T{\bf x}^i)) 
\end{eqnarray*}
quadratic loss is trying to close the distance, while logistic is trying to close the classification.
\subsubsection*{multiple classes}
Softmax function:
$$
P(y=c|{\bf x;W}) = \frac{\exp({\bf w}^T_c{\bf x})}{\sum_{c'=1}^{C}\exp({\bf w}^T_{c'}{\bf x})} = g_c({\bf x,W})
$$
Likelihood function of training sample: $({\bf y}^i,{\bf x}^i)$
$$
P({\bf y}^i|{\bf x}^i;{\bf w}) = \prod_{c=1}^{C}(g_c({\bf x, W}))^{{\bf y}_c^i}
$$
Optimization criterion:
$$
L({\bf W}) = -\sum_{i=1}^{N}\sum_{c=1}^{C}{\bf y}^i_c\log(g_c({\bf x,W}))
$$
\subsubsection*{Mapping data to higher-dimensional space}
while the data cannot be linear detected, the method is mapping the data to higher-dimensional space.
$$
L({\bf W'}) = -\sum_{i=1}^{N}\sum_{c=1}^{C}{\bf y}^i_c\log(g_c({\bf \phi(x),W}))
$$
\subsubsection*{Optimization to loss function}
{\large Gradient-based optimization}\\
\begin{eqnarray*}
\frac{\partial L({\bf w})}{\partial w_k} &=& -\sum_{i=1}^{N} \left[y^i\frac{1}{g({\bf w}^T{\bf x}^i)}\frac{\partial g({\bf w}^T{\bf x}^i)}{\partial w_k}+(1-y^i)\frac{1}{1-g({\bf w}^T{\bf x}^i)}(-\frac{\partial g({\bf w}^T{\bf x}^i)}{\partial w_k})\right]\\
&=& -\sum_{i=1}^{N}[y^i-g({\bf w}^T{\bf x}^i)]{\bf x}^i_k 
\end{eqnarray*}
This is for the non-linear system of binary classification.\\
\begin{eqnarray*}
&\nabla f({\bf x}) = \left[\begin{matrix}
\frac{\partial f}{\partial x_1}\\
\frac{\partial f}{\partial x_2}
\end{matrix}\right] \\
&Initial: \bf x_0 \\
&Update: {\bf x}_{i+1} = {\bf x}_i-\alpha\nabla f({\bf x_i}) \\
\end{eqnarray*}
It always works for \textbf{convex function}. But it is hard to set $\alpha$.
\large{second-order methods (newton method)}
First order Taylor series approximation:
$$
f(x) \approx f(a)+(x-a)f'(a)+e(x)
$$
Second order Taylor series approximation:
\begin{eqnarray*}
f(x) &=& f(a)+(x-a)f'(a)+\frac{1}{2}(x-a)^2f''(a)+e(x)\\
q'(x)&=&f'(x_i)+(x-x_i)f''(x_i) = 0\\
x_{i+1}&=&x_i-\frac{f'(x_i)}{f''(x_i)}
\end{eqnarray*}
{\large For the higher dimension}\\
\begin{eqnarray*}
f({\bf x}) &=& f({\bf x}_i)+({\bf x}-{\bf x}_i)\nabla f({\bf x}_i)+\frac{1}{2}({\bf x}-{\bf x}_i)^T{\bf H}({\bf x}-{\bf x}_i)\\
{\bf H_{i,j}} &=& \frac{\partial^2 f}{\partial x_i \partial x_j} \\
\nabla q(\bf x) &=& 0 \\
\nabla f({\bf x}_i)+({\bf x}-{\bf x}_i)^T{\bf H(x_i)} &=& 0 \\
{\bf x}_{i+1} &=& {\bf x}_i-({\bf H(x_i)})^{-1}\nabla f({\bf x}_i)
\end{eqnarray*}
Here is the hessian matrix for logistic loss function:
\begin{eqnarray*}
\frac{\partial^2 L({\bf w})}{\partial_{w_k}\partial_{w_j}} &=& \frac{\partial(-\sum_{i=1}^{N}[y^i-g({\bf w}^T{\bf x}^i)]{\bf x}^i_k )}{\partial w_j}\\
&=& \sum_{i=1}^{N}{\bf x}^i_k\frac{\partial g({\bf w}^T{\bf x}^i)}{\partial w_j}\\
&=& \sum_{i=1}^{N}{\bf x}^i_kg({\bf w}^T{\bf x}^i)(1-g({\bf w}^T{\bf x}^i)){\bf x}^i_j
\end{eqnarray*}

\subsection*{Perception}
Given $f(x)=sign(wx+b)$ as perception which is a discriminant. The distance between any point $x_0$ and the boundary is $\frac{|w\cdot x_0+b|}{||w||}$.\\
For the wrong classified data $(x_i,y_i)$: $-y_i(w\cdot x_i+b)>0$. Therefore, the distance between wrong classified data $(x_i,y_i)$ and the boundary is $-\frac{y_i(w\cdot x_i+b)}{||w||}$.\\
Then the loss function would be 
$$
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
$$
Where the $\frac{1}{||w||}$ is ignored. The reasons: 1) $||w||$ is only a scalar, which does not influence the vector $w$ direction 2)The perception training end condition is that loss $L(w,b)=0$, so the $||w||$ does not influence that.\\
For the training, the update would be:
$$
w\leftarrow w+\eta y_ix_i
$$$$
b\leftarrow b+\eta y_i
$$
\subsubsection*{Dual Property}
For fast calculation.
$$
w=\sum_{i=1}^{N}\alpha_iy_ix_i
$$$$
b=\sum_{i=1}^{N}\alpha_iy_i
$$
to be continued.
\subsection*{SVM}
Given the Discriminant: $y({\bf x}) = {\bf w}^T{\bf x}+b$
\subsubsection*{Functional Margins}
\begin{eqnarray*}
{\bf w}^T{\bf x}^i \gg0, if y^i = 1\\
{\bf w}^T{\bf x}^i \ll0, if y^i = -1\\
y^i({\bf w}^T{\bf x}^i) \gg 0
\end{eqnarray*}
According to that \textbf{${\bf w}$ is vertical to the boundary}(Due to ${\bf w}\cdot{\bf x}=-b \quad \forall {\bf x}$) and ${\bf x} = {\bf x}_{\perp}+\gamma\frac{\bf w}{\bf |w|}$, the distance between the point and the boundary would be $\gamma = \frac{{\bf w^T}{\bf x}+b}{|{\bf w}|}$
\subsubsection*{Support Vector}
The vectors (cases) that define the hyperplane are the support vectors.\\
Here we define that, for the positive support vector, ${\bf w}^T{\bf x}_++b=+1$ and ${\bf w}^T{\bf x}_-+b=-1$\\
In that way, the margin could be given as follow
$$
\frac{{\bf w}^T({\bf x}_+-{\bf x}_-)}{||\bf w||} = \frac{2}{||\bf w||}
$$
Then we needs to maximum the margin.
\begin{eqnarray*}
\max_{\bf w} \frac{2}{||\bf w||} \rightarrow \min_{\bf w}||{\bf w}||^2\\
\texttt{s.t. } y^i({\bf w}^T{\bf x}^i+b)\ge 1  \quad  \forall i
\end{eqnarray*}
\subsubsection*{Dual}
\begin{eqnarray*}
\min \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha^i\alpha^jy^iy^j<{\bf x}^i,{\bf x}^j> \\
\texttt{s.t. } y^i\left(\sum_{j=1}^{N}\alpha^jy^j<{\bf x}^i,{\bf x}^j>+b \right)\ge 1  \quad  \forall i
\end{eqnarray*}
\subsection*{Penalty constant}
\begin{eqnarray*}
\min_{{\bf w},\xi} ||{\bf w}||^2+C\sum_{i=1}^{N}\xi^i\\
s.t.: y^i({\bf w}^T{\bf x}^+b)\ge 1-\xi^i, \forall i\\
\xi^i\ge0, \forall i
\end{eqnarray*}
When misclassification when $\xi >1$.\\
$\sum_i\xi^i$: upper bound on number of errors. \\
C: Hyper-parameter.\\
Rewrite the first constraint: $y^ih_{{\bf w},b}({\bf x})\ge 1-\xi^i$ \\
Then combined all constraint:
\begin{eqnarray*}
\xi^i &=& [1-y^ih_{{\bf w},b}({\bf x})]_+\\
&=& \max(1-y^ih_{{\bf w},b}({\bf x}),0)
\end{eqnarray*}
Then the loss function would be:
\begin{eqnarray*}
L({\bf w})&=& \frac{1}{2}||{\bf w}||^2+C\sum_{i=1}^N \max(1-y^ih_{{\bf w},b}({\bf x}),0) \\
 &\propto& \lambda ||{\bf w}||^2+\sum_{i=1}^{N}\max(1-y^ih_{{\bf w},b}({\bf x}),0)
\end{eqnarray*}
where $\lambda||{\bf w}||^2$ is the regularizer and $\max(1-y^ih_{{\bf w},b}({\bf x}),0)$ is the additive loss. In that way, the Hinge loss($Yf(x)$) would be 0 when $Yf(x)\ge 0$. Compared to logistic regression and linear regression, it is a good feature.
\subsection*{Kernel}
$\phi$ is invariant to nuisance factors, sensitive to semantic variations (Encoder).\\
General idea: the original feature space can always be mapped  to some higher dimensional feature space where training set is trainable.\\
$\phi: \left(\begin{matrix}x_1\\x_2\end{matrix}\right)\rightarrow \left(\begin{matrix}r\\\theta\end{matrix}\right)$ would applied to the linearly separable in polar coordinates.\\
Kernel: $K({\bf x,y})= <\phi({\bf x}),\phi({\bf y})>$\\
Polynomial Kernel:$K({\bf x,y})=({\bf x^Ty}+1)^p$, which $\phi({\bf x}) = \left[\begin{matrix}
x_1^2\\x_2^2\\ \sqrt2x_1x_2 \\ \sqrt2x_1 \\ \sqrt2x_2 \\ 1
\end{matrix}\right]$\\
\subsubsection*{Condition for kernel trick}
Mercer Kernel:\\
1. Symmetric $k({\bf x}_i,{\bf x}_j) = k({\bf x}_j,{\bf x}_i)$\\
2. Positive definite, $\alpha^TK\alpha\ge0$ for all $\alpha \in \mathbb{R}^N$, where $K$ is the $N\times N$Gram matrix with entries $K_{ij}=k({\bf x}_i,{\bf x}_j)$ Gram matrix is a positive semidefinite matrix\\
Then $k(,)$ is a valid kernel.
\subsection*{SVM before kernel}
Optimization:
\begin{eqnarray*}
\min_\alpha\sum_{i=1}^N\sum_{j=1}^N\alpha^i\alpha^jy^iy^j<{\bf x}^i,{\bf x}^j>\\
s.t.: y^i\left(\sum_{j=1}^N\alpha^jy^j<{\bf x}^j,{\bf x}^i>+b\right)\ge 1, \forall i, {\bf \alpha}\in \mathbb{R}^N\rightarrow O(N^3)
\end{eqnarray*}
The classifier forms:
$$
f({\bf x}) = <{\bf w},{\bf x}>+b = \sum_{i=1}^N\alpha^iy^i<{\bf x}^i,{\bf x}>+b
$$
Linear kernel: $\bf K(x,y)=x^Ty$\\
Polynomial kernel: $\bf K(x,y) = (x^Ty+1)^p$\\
Radial Basis Function (a.k.a Gaussian) Kernel: ${\bf K(x,y)} = \exp(-\frac{1}{2\sigma^2}||{\bf x-y}||^2)$ \\
\subsubsection*{Radial Basis Function kernel expansion}
\begin{eqnarray*}
e^{-\gamma||{\bf x}_i-{\bf x}_j||^2} &=& e^{-\gamma({\bf x}_i-{\bf x}_j)^2} = e^{-\gamma{\bf x}_i^2+2\gamma{\bf x}_i{\bf x}_j-\gamma{\bf x}_j^2}\\
&=& e^{-\gamma{\bf x}_i^2-\gamma{\bf x}_j^2}(1+\frac{2\gamma{\bf x}_i{\bf x}_j}{1!}+\frac{(2\gamma{\bf x}_i{\bf x}_j)^2}{2!})+\frac{(2\gamma{\bf x}_i{\bf x}_j)^3}{3!}+\ldots) \\
&=& \phi({\bf x}_i)^T\phi({\bf x}_j)\\
where\quad \phi({\bf x}) &=& e^{-\gamma{\bf x}^2}[1,\sqrt{\frac{2\gamma}{1!}}{\bf x},\sqrt{\frac{(2\gamma)^2}{2!}}{\bf x}^2,\sqrt{\frac{(2\gamma)^3}{3!}}{\bf x}^3,\ldots]^T
\end{eqnarray*}
\subsection*{SVM after kernel}
Optimization:
\begin{eqnarray*}
\min_\alpha\sum_{i=1}^N\sum_{j=1}^N\alpha^i\alpha^jy^iy^jK({\bf x}^i,{\bf x}^j)\\
s.t.: y^i\left(\sum_{j=1}^N\alpha^jy^jK({\bf x}^i,{\bf x}^j)+b\right)\ge 1, \forall i, {\bf \alpha}\in \mathbb{R}^N\rightarrow O(N^3)
\end{eqnarray*}
The classifier forms:
\begin{eqnarray*}
f({\bf x}) &=& \sum_{i=1}^N\alpha^iy^iK({\bf x}^i,{\bf x}^j)+b\\
&=& \sum_{\{i:\alpha^i\ne0\}}w^iK({\bf x}^i,{\bf x}^j)+b, w^i = y^i\alpha^i
\end{eqnarray*}
Compare with general $f({\bf x}) = \sum_kw_k\phi_k({\bf x})$


\subsection*{SVM learning method SMO}

\subsection*{HoG/SIFT to image}




\section*{Tree(not in lecture)}
\subsection*{Decision tree}
Entropy measures the uncertain of the random variable $X$\\
Here the distribution of $X$ is $P(X=x_i)=p_i, i=1,2,\ldots,n$ \\
Entropy of $X$: $H(X) = -\sum_{i=1}^{n}p_i\log p_i$ \\
Conditional Entropy: $H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$ \\
Information Gain is the difference between empirical entropy and empirical conditional entropy.
$g(D,A) = H(D)-H(D|A)$\\
Information gain ratio:$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$
\subsection*{ID3}
using information gain
\subsection*{C4.5}
using information gain ratio
\subsection*{Regression Tree}
$f(x)=\sum_{m=1}^{M}c_mI(x\in R_m)$
\subsection*{Classification Tree}
K classes, and the probability is $p_k$\\
\begin{eqnarray*}
Gini(p) &=& \sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2\\
Gini(D,A) &=& \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2ï¼‰
\end{eqnarray*}
\section*{Ensemble learning}
\textbf{Generate a group of base-learners} which has higher accuracy when \textbf{combined}.\\
Consider the error, $\mathbb{E}_{COM} = \frac{1}{M}\mathbb{E}_{AV}$
\subsection*{Bagging}
pick subset of training data, then obtain weak learner.\\
Output final classifier by majority voting of the weak learner.\\
\subsection*{Boosting}
Pick subset of training data using a sampling distribution, obtain weak learner(use weak learner to update sampling distribution).\\
\subsubsection*{Adaboost}
Defines a classifier using an additive model(weighted voting):
$$
F(x) = \alpha_1f_1(x)+\alpha_2f_2(x)+\alpha_3f_3(x)+\ldots
$$
Given: $(x^i,y^i),x^i\in X, y^i\in{-1,1},i=1,\ldots,N$\\
Initialize: $D_1(i)=\frac{1}{N}$ distribution on the sample\\
For $t=1\ldots T$:\\
\ -Find classifier $h_t:X\rightarrow {-1,1} with smallest weighted error$\\
$$
\epsilon = \frac{\sum_{i=1}^{N}D_t^i[y^i\neq_t(x^i)]}{\sum_{i}{D_t^i}}
$$
\ -Update distribution: 
\begin{eqnarray*}
D_{t+1}^i &=& \frac{D_t^i}{Z_t}\times \{\begin{matrix}
\exp(-\alpha_t), {\rm if} \quad y^i = h_t(x^i)\\
\exp(\alpha_t), {\rm if} \quad y^i \neq h_t(x^i)
\end{matrix} \\
 &=& \frac{D_t^i}{Z_t}\exp(-\alpha_ty^ih_t(x^i)) \\
a_t &=& \frac{1}{2}\log\frac{1-\epsilon_t}{\epsilon}\\
Z_t &=& \sum_i D^i_t\exp(-\alpha_ty^ih_t(x^i))\\
{\rm Final\  classifier :} {\bf H}(x) &=&{\rm sign}(\sum_{t}\alpha_th_t(x)) 
\end{eqnarray*}
\large{Posterior}
\begin{eqnarray*}
\frac{\partial}{\partial f({\bf x})} \mathbb{E}[e^{-\tilde{y}f({\bf x})}|{\bf x}]&=& \frac{\partial}{\partial f({\bf x})}[p(\tilde{y}=1|{\bf x})e^{-f({\bf x})}+p(\tilde y=-1|{\bf x})e^{f({\bf x})}] \\
&=& -p(\tilde{y}=1|{\bf x})e^{-f({\bf x})}+p(\tilde{y}=-1|{\bf x})e^{f({\bf x})} \\
&=& 0 \Rightarrow \frac{p(\tilde{y}=1|{\bf x})}{p(\tilde{y}=-1|{\bf x})} =e^{2f({\bf x})}\\
f(x) &=& \frac{1}{2}\log(\frac{P(y=1|x)}{P(y=-1|x)})\\
\end{eqnarray*}
\end{document}




















