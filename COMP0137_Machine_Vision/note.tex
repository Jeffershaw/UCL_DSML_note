% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt,a4paper]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.
\usepackage[numbers, sort&compress]{natbib}
%\usepackage[square]{natbib}
%\usepackage{scicite}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{amsmath}
%\usepackage{CJKutf8}
% Use times if you have the font installed; otherwise, comment out the
% following line.

\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.
\geometry{a4paper,scale=0.8}
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 20cm 
\textheight 21cm
\footskip 1.0cm
\geometry{top=1.5cm}
%The next command sets up an environment for the abstract to your paper.
\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}
\renewcommand\refname{References and Notes}
\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Machine Vision} 

%\author{Jiafeng Shou} 
\time 0
\begin{document} 
% Double-space the manuscript.

\baselineskip24pt
% Make the title.
\maketitle 
%\tableofcontents

\section{Lecture 1}
% \begin{eqnarray}
% norm({\rm {\bf W}_i(\theta_j)})=\sqrt{x_1^2+x_2^2+\ldots+x_n^2}\\
% {\rm {\bf \bar W}_i(\theta_j)} = \frac{\rm {\bf W}_i(\theta_j)}{norm({\rm {\bf W}_i(\theta_j)})}
% \end{eqnarray}
%$$\int_{2}^{3}x^2dx$$
\subsection*{Bernoulli Distribution}

$$
Pr(x) = {\lambda}^x(1-\lambda)^{1-x}, \lambda \in =[0,1], x\in\{0,1\}
$$
$$
Pr(x) = Bern_x[\lambda]
$$
\subsection*{Beta Distribution}
$$
Pr(\lambda) = \frac{\Gamma[\alpha+\beta]}{\Gamma[\alpha]\Gamma[\beta]}\lambda^{\alpha-1}(1-\lambda)^{\beta-1},\alpha,\beta>0
$$
$$
\Gamma(z) = \int_{0}^{\infty}t^{z-1}c^{-t}dt=(z-1)!
$$
$$
E[\lambda] = \frac{\alpha}{\alpha+\beta}
$$
$$
B(p,q) = \frac{q-1}{p+q+1}B(p,q-1)
$$
$\alpha,\beta$ decide the coin fact $\lambda$
\subsection*{Categorical Distribution}
$$
Pr(x=k)=\lambda_k
$$
$$
Pr(x) = Cat_x[{\bf \lambda}]
$$
\subsection*{Dirichlet Distribution}
$$
Pr(\lambda_1\ldots\lambda_K) = \frac{\Gamma[\sum_{k=1}^{K}]\alpha_k}{\prod_{k=1}^{K}\Gamma[\alpha_k]}\prod_{k=1}^{K}\lambda_k^{\alpha_k-1}
$$
$$
Pr(\lambda_1\ldots\lambda_K) = {\rm Dir}_{\lambda_1\ldots\lambda_K}[\alpha_1,\alpha_2\ldots,\alpha_K]
$$
\subsection*{Univariate Normal Distribution}
$$
Pr(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp[-0.5(x-\mu)^2/\sigma^2]
$$
$$
Pr(x) = {\rm Norm}_x[\mu,\sigma^2]
$$
\subsection*{Normal Inverse Gamma Distribution}
$$
Pr(\mu,\sigma^2) = \frac{\sqrt{\gamma}\beta^{\alpha}}{\sigma\sqrt{2\pi}\Gamma[\alpha]}(\frac{1}{\sigma^2})^{\alpha+1}\exp[-\frac{2\beta+\gamma(\delta-\mu)^2}{2\sigma^2}]
$$
$$
Pr(\mu,\sigma^2) = {\rm NormInvGam}_{\mu,\sigma^2}[\alpha,\beta,\gamma,\delta]
$$
\subsection*{Multivariate Normal Distribution}
$$
Pr({\bf x})=\frac{1}{(2\pi)^{D/2}|\sum|^{1/2}}\exp[-0.5({\bf x- \mu})^T{\sum}^-1(\bf x-\mu)]
$$
\subsection*{Normal Inverse Wishart}
$$
Pr(\mu,\Sigma) = \frac{\gamma^{D/2}|\Psi|^{\alpha/2}|\Sigma|^{-\frac{\alpha+D+2}{2}}}{(2\pi)^{D/2}2^{\frac{\alpha D}{2}}\Gamma_D(\frac{\alpha}{2})}\exp\{-\frac{1}{2}(Tr(\Psi\Sigma^{-1}))+\gamma(\mu-\delta)^T\Sigma^{-1}(\mu-\delta)\}
$$
\subsection*{Conjugate Distribution and Conjugate prior}
Conjugate Distribution is between prior and posterior\\
Prior is the conjugate prior of the likelihood function.
\section{Fitting model}
\subsection*{maximum likelihood}
\subsubsection*{Fitting}
\begin{eqnarray*}
\hat{\theta} &=& argmax_{(\theta)}[Pr({\bf x_{1\ldots I}}|\theta)] \\
&=& argmax_{(\theta)}[\prod_{i=1}^{I}Pr({\bf x_{i}}|\theta)]
\end{eqnarray*}
\subsubsection*{Predictive Density}
Evaluate new data point $\bf x^*$ under probability distribution $Pr({\bf x^*}|\hat{\theta})$ with best parameter.
\subsection*{maximum a posteriori}
\subsubsection*{Fitting}
\begin{eqnarray*}
\hat{\theta} &=& argmax_{(\theta)}[Pr(\theta|{\bf x_{1\ldots I}})] \\
&=& argmax_{(\theta)}\left[\frac{Pr({\bf x_{1\ldots I}}|\theta)Pr(\theta)}{Pr({\bf x_{1\ldots I}})}\right]\\
&=& argmax_{(\theta)}\left[\frac{\prod_{i=1}^{I}Pr({\bf x_i}|\theta)Pr(\theta)}{Pr({\bf x_{1\ldots I}})}\right]\\
\hat{\theta} &=& argmax_{(\theta)}\left[Pr({\bf x_i}|\theta)Pr(\theta)\right]
\end{eqnarray*}
\subsubsection*{Predictive}
Evaluate new data point $\bf x^*$ under probability distribution $Pr({\bf x^*}|\hat{\theta})$ with best parameter.
\subsection*{bayesian approach}
\subsubsection*{Fitting}
$$
Pr(\theta|{\bf x_{1\ldots I}}) = \frac{(\prod_{i=1}^{I}Pr({\bf x_i}|\theta))Pr(\theta)}{Pr({\bf x_{1\ldots I}})}
$$
The difference between bayesian approach and MAP is that MAP takes the maximum value, while bayesian approach takes the distribution.
\subsubsection*{Predictive}
$$
Pr({\bf x^*|{\bf x_{1\ldots I}}}) = \int Pr({\bf x^*|\theta})Pr(\theta|{\bf x_{1\ldots I}})d\theta
$$
Confusion: the formula should be $\int Pr({\bf x^*|\theta, x_{1\ldots I}})Pr(\theta|{\bf x_{1\ldots I}})d\theta$. Given the $\theta$, it considers ${\bf x_{1\ldots I}}$ and $x^*$ are independent. 
\subsection*{Multivariate Normal Distribution}
If ${\bf x_1, x_2\ldots x_n}$ are independent, the covariance matrix would be 
$$
\Sigma = \left[\begin{matrix} 
\sigma_1^2 & 0 & \ldots & 0\\
0 & \sigma_2^2 & \ldots & 0\\
\vdots & \vdots & \ddots & 0 \\
0 & 0 & \ldots & \sigma_n^2
\end{matrix}
\right]
$$
Therefore, while ${\bf x_1, x_2\ldots x_n}$ are dependent, the covariance matrix could be decomposed into rotation matrix and diagonal:
$$
\Sigma_{full} = {\bf R}^T\Sigma_{diag}'{\bf R}
$$
\subsubsection*{Marginal Distribution}
$$
u_i = u_i
$$$$
\Sigma_i= \Sigma_{ii} 
$$
\subsubsection*{Conditional Distribution}
$$
u_{i|j} = u_i+\Sigma_{ij}\Sigma_{jj}^{-1}(x_j-u_j)
$$$$
\Sigma_{i|j} = \Sigma_{jj}-\Sigma_{ij}^T\Sigma_{ii}^{-1}\Sigma_{ij}
$$
\subsubsection*{Product of two normals}
\begin{eqnarray*}
{\rm Norm}_{\bf x}[{\bf a, A}]{\rm Norm}_{\bf x}[{\bf b, B}] &=& k \cdot {\rm Norm}_{\bf x}{\bf [(A^{-1}+B^{-1})^{-1}(A^{-1}a+B^{-1}b),(A^{-1}+B^{-1})^{-1}]}\\
k &=&{\rm Norm}_{\bf a}[{\bf b, A+B}]
\end{eqnarray*}
\subsubsection*{change of variables}
$$
{\rm Norm}_{\bf x}[{\bf Ay+b,\Sigma}] = k\cdot{\rm Norm}_{\bf y}[{\bf A'x+b',\Sigma'}]
$$
where 
\begin{eqnarray*}
\bf A' &=& \Sigma'A^T\Sigma^{-1}\\
b' &=& -\Sigma'A^T\Sigma^{-1}b \\
\Sigma &=& (A^T\Sigma^{-1}A)^{-1}\\
\end{eqnarray*}
\section*{Learning and Inference}
The observe measured data, $\bf x$\\
Draw inference from it about the state of world, $\bf w$\\
If $\bf w$ is continuous, call this regression.\\
If $\bf w$ is discrete, call this classification.\\
To compute the probability distribution $Pr({\bf w|x})$, we need: a model(relates visual data $\bf x$ and $\bf w$, the relationships depends on parameter $\bf \theta$), a learning algorithm(fits parameter $\bf \theta$ from paired training examples $\bf x_i,w_i$), an inference algorithm (use model to return $Pr({\bf w|x})$ given new observed data $\bf x$)
\subsection*{Types of Model}
\subsubsection*{1. Model contingency of the world on the data $Pr({\bf w|x})$ (Discriminative models)}
1. Choose an appropriate from form for $Pr(\bf w)$\\
2. Make parameters a function of $\bf x$\\
3. Function takes parameters $\theta$ that define its shape.\\
Inference: evaluate $\bf Pr(w|x)$
\subsubsection*{2. Model joint occurrence of the world and data $Pr({\bf x,w})$ （Generative models）}
1. COncatenate $\bf x$ and $\bf w$ to make $\bf z=[x^T w^T]$\\
2. Model of pdf of $\bf z$\\
3. Pdf takes parameter $\theta$ that define its shape\\
Inference: compute $Pr({\bf w|x})$ using Bayes rule.
$$
Pr({\bf w|x}) = \frac{Pr({\bf x,w})}{Pr({\bf x})}= \frac{Pr({\bf x,w})}{\int Pr({\bf x,w})d{\bf w}}
$$
\subsubsection*{3. Model contingency of data on the world $Pr({\bf x|w})$ (Generative models)}
1. Choose an appropriate form for $Pr({\bf x})$\\
2. Make parameters a function of $\bf w$\\
3. Function takes parameter $\theta$ that define its shape.\\
Inference: define prior $Pr({\bf w})$ and then compute $Pr({\bf w|x})$ using Bayes' rule.
$$
Pr({\bf w|x}) = \frac{Pr({\bf x|w})Pr({\bf w})}{\int Pr(\bf x|w)Pr({\bf w})d{\bf w}}
$$

\end{document}




















