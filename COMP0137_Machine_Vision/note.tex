% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt,a4paper]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.
\usepackage[numbers, sort&compress]{natbib}
%\usepackage[square]{natbib}
%\usepackage{scicite}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{amsmath}
%\usepackage{CJKutf8}
% Use times if you have the font installed; otherwise, comment out the
% following line.

\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.
\geometry{a4paper,scale=0.8}
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 20cm 
\textheight 21cm
\footskip 1.0cm
\geometry{top=1.5cm}
%The next command sets up an environment for the abstract to your paper.
\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}
\renewcommand\refname{References and Notes}
\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Machine Vision} 

%\author{Jiafeng Shou} 
\time 0
\begin{document} 
% Double-space the manuscript.

\baselineskip24pt
% Make the title.
\maketitle 
%\tableofcontents

\section{Lecture 1}
% \begin{eqnarray}
% norm({\rm {\bf W}_i(\theta_j)})=\sqrt{x_1^2+x_2^2+\ldots+x_n^2}\\
% {\rm {\bf \bar W}_i(\theta_j)} = \frac{\rm {\bf W}_i(\theta_j)}{norm({\rm {\bf W}_i(\theta_j)})}
% \end{eqnarray}
%$$\int_{2}^{3}x^2dx$$
\subsection*{Bernoulli Distribution}

$$
Pr(x) = {\lambda}^x(1-\lambda)^{1-x}, \lambda \in =[0,1], x\in\{0,1\}
$$
$$
Pr(x) = Bern_x[\lambda]
$$
\subsection*{Beta Distribution}
$$
Pr(\lambda) = \frac{\Gamma[\alpha+\beta]}{\Gamma[\alpha]\Gamma[\beta]}\lambda^{\alpha-1}(1-\lambda)^{\beta-1},\alpha,\beta>0
$$
$$
\Gamma(z) = \int_{0}^{\infty}t^{z-1}c^{-t}dt=(z-1)!
$$
$$
E[\lambda] = \frac{\alpha}{\alpha+\beta}
$$
$$
B(p,q) = \frac{q-1}{p+q+1}B(p,q-1)
$$
$\alpha,\beta$ decide the coin fact $\lambda$
\subsection*{Categorical Distribution}
$$
Pr(x=k)=\lambda_k
$$
$$
Pr(x) = Cat_x[{\bf \lambda}]
$$
\subsection*{Dirichlet Distribution}
$$
Pr(\lambda_1\ldots\lambda_K) = \frac{\Gamma[\sum_{k=1}^{K}]\alpha_k}{\prod_{k=1}^{K}\Gamma[\alpha_k]}\prod_{k=1}^{K}\lambda_k^{\alpha_k-1}
$$
$$
Pr(\lambda_1\ldots\lambda_K) = {\rm Dir}_{\lambda_1\ldots\lambda_K}[\alpha_1,\alpha_2\ldots,\alpha_K]
$$
\subsection*{Univariate Normal Distribution}
$$
Pr(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp[-0.5(x-\mu)^2/\sigma^2]
$$
$$
Pr(x) = {\rm Norm}_x[\mu,\sigma^2]
$$
\subsection*{Normal Inverse Gamma Distribution}
$$
Pr(\mu,\sigma^2) = \frac{\sqrt{\gamma}\beta^{\alpha}}{\sigma\sqrt{2\pi}\Gamma[\alpha]}(\frac{1}{\sigma^2})^{\alpha+1}\exp[-\frac{2\beta+\gamma(\delta-\mu)^2}{2\sigma^2}]
$$
$$
Pr(\mu,\sigma^2) = {\rm NormInvGam}_{\mu,\sigma^2}[\alpha,\beta,\gamma,\delta]
$$
\subsection*{Multivariate Normal Distribution}
$$
Pr({\bf x})=\frac{1}{(2\pi)^{D/2}|\sum|^{1/2}}\exp[-0.5({\bf x- \mu})^T{\sum}^-1(\bf x-\mu)]
$$
\subsection*{Normal Inverse Wishart}
$$
Pr(\mu,\Sigma) = \frac{\gamma^{D/2}|\Psi|^{\alpha/2}|\Sigma|^{-\frac{\alpha+D+2}{2}}}{(2\pi)^{D/2}2^{\frac{\alpha D}{2}}\Gamma_D(\frac{\alpha}{2})}\exp\{-\frac{1}{2}(Tr(\Psi\Sigma^{-1}))+\gamma(\mu-\delta)^T\Sigma^{-1}(\mu-\delta)\}
$$
\subsection*{Conjugate Distribution and Conjugate prior}
Conjugate Distribution is between prior and posterior\\
Prior is the conjugate prior of the likelihood function.
\section{Fitting model}
\subsection*{maximum likelihood}
\subsubsection*{Fitting}
\begin{eqnarray*}
\hat{\theta} &=& argmax_{(\theta)}[Pr({\bf x_{1\ldots I}}|\theta)] \\
&=& argmax_{(\theta)}[\prod_{i=1}^{I}Pr({\bf x_{i}}|\theta)]
\end{eqnarray*}
\subsubsection*{Predictive Density}
Evaluate new data point $\bf x^*$ under probability distribution $Pr({\bf x^*}|\hat{\theta})$ with best parameter.
\subsection*{maximum a posteriori}
\subsubsection*{Fitting}
\begin{eqnarray*}
\hat{\theta} &=& argmax_{(\theta)}[Pr(\theta|{\bf x_{1\ldots I}})] \\
&=& argmax_{(\theta)}\left[\frac{Pr({\bf x_{1\ldots I}}|\theta)Pr(\theta)}{Pr({\bf x_{1\ldots I}})}\right]\\
&=& argmax_{(\theta)}\left[\frac{\prod_{i=1}^{I}Pr({\bf x_i}|\theta)Pr(\theta)}{Pr({\bf x_{1\ldots I}})}\right]\\
\hat{\theta} &=& argmax_{(\theta)}\left[Pr({\bf x_i}|\theta)Pr(\theta)\right]
\end{eqnarray*}
\subsubsection*{Predictive}
Evaluate new data point $\bf x^*$ under probability distribution $Pr({\bf x^*}|\hat{\theta})$ with best parameter.
\subsection*{bayesian approach}
\subsubsection*{Fitting}
$$
Pr(\theta|{\bf x_{1\ldots I}}) = \frac{(\prod_{i=1}^{I}Pr({\bf x_i}|\theta))Pr(\theta)}{Pr({\bf x_{1\ldots I}})}
$$
The difference between bayesian approach and MAP is that MAP takes the maximum value, while bayesian approach takes the distribution.
\subsubsection*{Predictive}
$$
Pr({\bf x^*|{\bf x_{1\ldots I}}}) = \int Pr({\bf x^*|\theta})Pr(\theta|{\bf x_{1\ldots I}})d\theta
$$
Confusion: the formula should be $\int Pr({\bf x^*|\theta, x_{1\ldots I}})Pr(\theta|{\bf x_{1\ldots I}})d\theta$. Given the $\theta$, it considers ${\bf x_{1\ldots I}}$ and $x^*$ are independent. 
\subsection*{Multivariate Normal Distribution}
If ${\bf x_1, x_2\ldots x_n}$ are independent, the covariance matrix would be 
$$
\Sigma = \left[\begin{matrix} 
\sigma_1^2 & 0 & \ldots & 0\\
0 & \sigma_2^2 & \ldots & 0\\
\vdots & \vdots & \ddots & 0 \\
0 & 0 & \ldots & \sigma_n^2
\end{matrix}
\right]
$$
Therefore, while ${\bf x_1, x_2\ldots x_n}$ are dependent, the covariance matrix could be decomposed into rotation matrix and diagonal:
$$
\Sigma_{full} = {\bf R}^T\Sigma_{diag}'{\bf R}
$$
\subsubsection*{Marginal Distribution}
$$
u_i = u_i
$$$$
\Sigma_i= \Sigma_{ii} 
$$
\subsubsection*{Conditional Distribution}
$$
u_{i|j} = u_i+\Sigma_{ij}\Sigma_{jj}^{-1}(x_j-u_j)
$$$$
\Sigma_{i|j} = \Sigma_{jj}-\Sigma_{ij}^T\Sigma_{ii}^{-1}\Sigma_{ij}
$$
\subsubsection*{Product of two normals}
\begin{eqnarray*}
{\rm Norm}_{\bf x}[{\bf a, A}]{\rm Norm}_{\bf x}[{\bf b, B}] &=& k \cdot {\rm Norm}_{\bf x}{\bf [(A^{-1}+B^{-1})^{-1}(A^{-1}a+B^{-1}b),(A^{-1}+B^{-1})^{-1}]}\\
k &=&{\rm Norm}_{\bf a}[{\bf b, A+B}]
\end{eqnarray*}
\subsubsection*{change of variables}
$$
{\rm Norm}_{\bf x}[{\bf Ay+b,\Sigma}] = k\cdot{\rm Norm}_{\bf y}[{\bf A'x+b',\Sigma'}]
$$
where 
\begin{eqnarray*}
\bf A' &=& \Sigma'A^T\Sigma^{-1}\\
b' &=& -\Sigma'A^T\Sigma^{-1}b \\
\Sigma &=& (A^T\Sigma^{-1}A)^{-1}\\
\end{eqnarray*}
\section*{Learning and Inference}
The observe measured data, $\bf x$\\
Draw inference from it about the state of world, $\bf w$\\
If $\bf w$ is continuous, call this regression.\\
If $\bf w$ is discrete, call this classification.\\
To compute the probability distribution $Pr({\bf w|x})$, we need: a model(relates visual data $\bf x$ and $\bf w$, the relationships depends on parameter $\bf \theta$), a learning algorithm(fits parameter $\bf \theta$ from paired training examples $\bf x_i,w_i$), an inference algorithm (use model to return $Pr({\bf w|x})$ given new observed data $\bf x$)
\subsection*{Types of Model}
\subsubsection*{1. Model contingency of the world on the data $Pr({\bf w|x})$ (Discriminative models)}
1. Choose an appropriate from form for $Pr(\bf w)$\\
2. Make parameters a function of $\bf x$\\
3. Function takes parameters $\theta$ that define its shape.\\
Inference: evaluate $\bf Pr(w|x)$
\subsubsection*{2. Model joint occurrence of the world and data $Pr({\bf x,w})$ （Generative models）}
1. COncatenate $\bf x$ and $\bf w$ to make $\bf z=[x^T w^T]$\\
2. Model of pdf of $\bf z$\\
3. Pdf takes parameter $\theta$ that define its shape\\
Inference: compute $Pr({\bf w|x})$ using Bayes rule.
$$
Pr({\bf w|x}) = \frac{Pr({\bf x,w})}{Pr({\bf x})}= \frac{Pr({\bf x,w})}{\int Pr({\bf x,w})d{\bf w}}
$$
\subsubsection*{3. Model contingency of data on the world $Pr({\bf x|w})$ (Generative models)}
1. Choose an appropriate form for $Pr({\bf x})$\\
2. Make parameters a function of $\bf w$\\
3. Function takes parameter $\theta$ that define its shape.\\
Inference: define prior $Pr({\bf w})$ and then compute $Pr({\bf w|x})$ using Bayes' rule.
$$
Pr({\bf w|x}) = \frac{Pr({\bf x|w})Pr({\bf w})}{\int Pr(\bf x|w)Pr({\bf w})d{\bf w}}
$$
\subsection*{Bessel correction}
$s^2 = (\frac{n}{n-1})s_n^2$
working this later.
\section*{Learning and inference}
\subsection*{Mixture of Model}
$$
Pr({\bf x|\theta}) = \sum_{k=1}^KPr({\bf x},h=k|\theta)
$$

\subsection*{Mixture of Gaussian}
$$
Pr({\bf x|\theta}) = \sum_{k=1}^{K}\lambda_k{\rm Norm}_{\bf x}[\mu_k,\Sigma_k]
$$
Usually, the dimension would be smaller than sample.
\subsection*{Hidden variables}
\begin{eqnarray*}
Pr({\bf x}) &=& \int Pr({\bf x,h})d{\bf h} \\
Pr({\bf x|\theta}) &=& \int Pr({\bf x,h|\theta})d{\bf h}\\
\hat\theta  &=& {\rm argmax}_\theta\left[\sum_{i=1}^{\bf I}log[\int Pr({\bf x}_i,{\bf h}_i|\theta)d{\bf h}_i]\right] \\
B[\{q_i({\bf h}_i)\},\theta] &=& \sum_{i=1}^{\bf I}\int q_i({\bf h}_i)\log[\frac{Pr({\bf x,h}_i|\theta)}{q_i({\bf h}_i)}]d{\bf h}_{1\ldots I}\le \sum_{i=1}^{\bf I}log[\int Pr({\bf x}_i,{\bf h}_i|\theta)d{\bf h}_i]
\end{eqnarray*}
\subsection*{Lower bound}
Because the log of sum is hard to derivate to 0.\\
According to Jensen's inequality when $f(x)$ is a convex function:
$$
f({\bf E[X]}) \le {\bf E}[f({\bf X})] 
$$
For the concave function:
$$
f({\bf E[X]}) \ge {\bf E}[f({\bf X})] 
$$
Therefore the lower bound holds:
\begin{eqnarray*}
\log({\bf E}\left[\frac{Pr({\bf x,h}_i|\theta)}{q({\bf h}_i)}\right]) &\ge& {\bf E}\left[log(\frac{Pr({\bf x,h}_i|\theta)}{q({\bf h}_i)})\right] \\
\log(\int \left[\frac{Pr({\bf x,h}_i|\theta)}{q({\bf h}_i)}q({\bf h}_i)\right]d{\bf h}_i) &\ge& \int \left[q({\bf h}_i)log(\frac{Pr({\bf x,h}_i|\theta)}{q({\bf h}_i)})\right]d{\bf h}_i \\
\sum_{i=1}^{\bf I}log[\int Pr({\bf x}_i,{\bf h}_i|\theta)d{\bf h}_i] &\ge & \sum_{i=1}^{\bf I}\int q_i({\bf h}_i)\log[\frac{Pr({\bf x,h}_i|\theta)}{q_i({\bf h}_i)}]d{\bf h}_{1\ldots I}
\end{eqnarray*}
Where $\log$ function is the $f({\bf X})$, and $q({\bf h}_i)$ is $Pr({\bf h}_i|{\bf x}_i,\theta^{[t]})$
\subsection*{E-Step}
Maximize the bound w.r.t. distribution $q({\bf h}_i)$
$$
\hat q_i({\bf h}_i) = Pr({\bf h}_i|{\bf x}_i,{\bf \theta}^{[t]}) = \frac{ Pr({\bf x}_i|{\bf h}_i,{\bf \theta}^{[t]})Pr({\bf h}_i|\theta^{[t]})}{Pr({\bf x}_i)}
$$
\subsection*{M-Step}
Maximize bound w.r.t parameter $\theta$
$$
\hat \theta^{[t+1]} = {\rm argmax}_{\theta}\left[\sum_{i=1}^{I}\int \hat q_i({\bf h}_i)\log [Pr({\bf x}_i,{\bf h}_i|\theta)] d{\bf h}_i\right]
$$
\subsection*{E-step of MoG}
\begin{eqnarray*}
Pr(h_i=k|{\bf x}_i,\theta^{[t]}) &=& \frac{Pr({\bf x}_i|h_i=k,\theta^{[t]})Pr(h_i=k,\theta^{[t]})}{\sum_{j=1}^{K}Pr({\bf x}_i|h_i=j,\theta^{[t]})Pr(h_i=j,\theta^{[t]})}\\
&=& \frac{\lambda_k {\rm Norm}_{\bf x_i}[\mu_k,\Sigma_k]}{\sum_{j=1}^{K}\lambda_j {\rm Norm}_{\bf x_i}[\mu_j,\Sigma_j]}\\
&=& r_{i,k}
\end{eqnarray*}
\subsection*{M-step of MoG}
Take derivative, equal to zero and solve:
\begin{eqnarray*}
\lambda_k^{[t+1]} &=& \frac{\sum_{i=1}^{I} r_{i,k}}{\sum_{j=1}^{K}\sum_{i=1}^{I}r_{i,j}} \\
\mu_k^{[t+1]} &=& \frac{\sum_{i=1}^{I}r_{i,k}{\bf x}_i}{\sum_{i=1}^{I}r_{i,k}}\\
\Sigma_k^{[t+1]} &=& \frac{\sum_{i=1}^{I}r_{i,k}({\bf x}_i-\mu_k^{[t+1]})({\bf x}_i-\mu_k^{[t+1]})^T}{\sum_{i=1}^{I}r_{i,k}} 
\end{eqnarray*}
\subsection*{Student t-distribution}
not willing to write, seems not important\\
compared to MoG, it is more robustness.
\subsection*{Factor analysis}
not willing to write, seems not important\\
compared to MoG, it is applied when dimension is larger than sample. Or the covariance cannot be invertible.
\section*{Regression}
\subsection*{Linear Regression}
The core idea is to regard the error as the normal distribution.
$$
Pr({\bf w}|{\bf X,\theta}) = {\rm Norm}_{\bf w}[{\bf X}^T\phi,\sigma^2{\bf I}]
$$
Use the maximum likelihood to calculate, then take the derivative, set result to 0 and re-arrange:
\begin{eqnarray*}
\hat \phi &=& ({\bf XX}^T)^{-1}{\bf X}{\bf w}\\
\hat \sigma^2 &=& \frac{({\bf w}-{\bf X}^T\phi)^T({\bf w-{\bf X}^T\phi})}{\bf I}
\end{eqnarray*}
\subsection*{Linear Regression in Bayesian}
Besides max likelihood, the bayesian model could be applied:
Likelihood:
$$
Pr({\bf w}|{\bf X,\theta}) = {\rm Norm}_{\bf w}[{\bf X}^T\phi,\sigma^2{\bf I}]
$$
Prior:
$$
Pr(\phi) = {\rm Norm}_\phi[0,\sigma_p^2{\bf I}]
$$
Bayes rules:
$$
Pr(\phi|{\bf X,w}) = \frac{Pr({\bf w|X,\phi})Pr(\phi|{\bf X})}{Pr({\bf w|X})}
$$
In that case, it could be concluded as follow:
$$
Pr(\phi|{\bf X,w}) ={\rm Norm}_{\phi}[\frac{1}{\sigma^2}{\bf A}^{-1}{\bf Xw},{\bf A}^{-1}] 
$$
Where ${\bf A}=\frac{1}{\sigma^2}{\bf XX}^T+\frac{1}{\sigma_p^2}{\bf I}$.\\
Inference could be calculated as following:
\begin{eqnarray*}
Pr(w^*|{\bf x}^*,{\bf X,w}) &=& \int Pr(w^*|{\bf x}^*,\phi)Pr(\phi|{\bf X,w})d\phi\\
 &=& {\rm Norm}_{w^*}[\frac{1}{\sigma^2}{{\bf x}^*}^T{\bf A^{-1}Xw},{\bf x^*}^T{\bf A^{-1}x^*}+\sigma^2]
\end{eqnarray*}
where $\bf A^{-1}$ is hard to calculated when the dimension is large, then directly calculate the $\bf A^{-1}$.\\
For the variance fitting, using a marginal distribution to calculate the maximum likelihood:
$$
Pr({\bf w|X,\sigma^2}) = \int Pr({\bf w|X,\phi,\sigma^2})Pr(\phi)d\phi = {\rm Norm}_{\bf w}[0,\sigma_p^2{\bf X}^T{\bf X+\sigma^2I}]
$$
\subsection*{Gaussian Process Regression}
$$
Pr(w_i|{\bf x}_i,\theta) = {\rm Norm}_{w_i}[\phi^T{\bf z}_i,\sigma^2]
$$
The difference between non-linear regression is using ${\bf z}_i$ to substitute $\bf x_i$. The other steps are similar.
\subsection*{Kernel regression}
substitute $\bf Z_i^TZ_i$ as a kernel ${\bf K[X,X]}$. The advantage is that not waste time on calculating the high dimension $\bf z$. The specific example could refer to the Gaussian kernel, the $\bf z$ of Gaussian kernel is infinite dimension. As a kernel, it is calculated fast.
\subsection*{Sparse Linear regression}
Perhaps not every dimension of the data $\bf x$ is informative
A sparse solution forces some of the coefficients in $\phi$ to be zero.
The difference between Sparse linear regression and linear regression, here, we applied the t-distribution as the prior as the distribution of $\phi$.\\
The basic idea for this regression is that, t-distribution has a better robustness in data point selection. Then after applying t-distribution as the $\phi$ distribution, in the fitting phase, fitted $\phi$ has sparse property. 
\begin{eqnarray*}
Pr(\phi) &=& \prod_{D}^{d=1}{\rm Stud}_{\phi_d}[0,1,\nu]\\
 &=& \prod_{d=1}^{D}\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{\phi^2_d}{\nu})^{-(\nu+1)/2}
\end{eqnarray*}
For every single t-distribution, it could be regarded as a mixture of Gaussian distribution. Therefore, it could be expressed as follow (hidden variable):
\begin{eqnarray*}
Pr(\phi) &=& \prod_{d=1}^{D}\int {\rm Norm}_{\phi_d}[0,1/h_d]{\rm Gam}_{h_d}[\nu/2,\nu/2]dh_d \\
&=& \int {\rm Norm}_{\phi_d}[0,{\bf H}^{-1}]\prod_{d=1}^{D}{\rm Gam}_{h_d}[\nu/2,\nu/2]d{\bf H}
\end{eqnarray*}
Then, according to bayesian:
$$
Pr({\bf w|X,\sigma^2}) \approx \max_{\bf H}[{\rm Norm}_{\bf w}[0,{\bf X^TH^{-1}X+\sigma^2I}]\prod_{d=1}^{D}{\rm Gam}_{h_d}[\nu/2,\nu/2]]
$$
The specific result could refer to ppt08 page 44.\\
However, it is hard to handle high dimension.
\subsection*{Dual Linear Regression}
This model could be regarded as SVM regression in bayesian framework.\\
In linear SVR, the regressor is:
$$
y = \sum_{i=1}^{N}(a_i-a_i^*)<x_i,x>+b
$$ 
where $a_i$ is the upper bound penalty factor Lagrange multiplier, and $a_i^*$ is lower penalty factor Lagrange multiplier. Then $(a_i-a_i^*)$ could be regarded as a new coefficient. Then introduce the dual linear regression. The specific SVR tutorial URL could be found in http://kernelsvm.tripod.com/.
The idea is that $\phi$ could be represented as
$$
\phi = {\bf X\psi} =  \sum_{i=1}^{N}(a_i-a_i^*)x_i
$$ 
Then dual linear regression could be represented as :
$$
Pr({\bf w|X,\theta}) = {\rm Norm}_{\bf w}[{\bf X}^T{\bf X\psi},\sigma^2{\bf I}]
$$
The fitting and inferencing are the same as linear regression above.
\subsection*{Relevance Vector Machine}
The idea is to combine dual regression and sparsity.
$$
Pr({\bf w|X,\theta}) = {\rm Norm}_{\bf w}[{\bf X^TX\psi},\sigma^2{\bf I}] 
$$$$
Pr(\psi) = \prod_{i=1}^{I}{\rm Stud}_{\psi_i}[0,1,\nu]
$$
\section*{Classification}
\subsection*{Logistic Regression}
$$
Pr(w|\phi_0,\phi,{\bf x}) = {\rm Bern}_w[\sigma(\phi^T{\bf x})]
$$
If use the maximum likelihood to learning, the gradient would be 
$$
\frac{\partial L}{\partial \phi} = -\sum_{i=1}^{I}(\frac{1}{1+\exp[-\phi^T{\bf x}_i]}-w_i){\bf x}_i=-\sum_{i=1}^{I}({\rm sig}[a_i]-w_i){\bf x}_i
$$
we cannot get the expression for $\phi$ in term of $x$ and $w$.
Therefore, the goal is to optimize that
$$
\hat \theta = {\rm argmin}_\theta[f[\theta]]
$$
If a function is \large{convex}, then it has only a single minimum.\\
\subsubsection*{Gradient Based Optimization}
1. Choose a search direction $\bf s$ based on the local properties of the function. \\
2. Perform an intensive search along the chosen direction. This is called line search:
$$
\hat \lambda = {\rm argmin}_{\lambda}[f[\theta^{[t]}+\lambda{\bf s}]]
$$
Then set
$$
\theta^{[t+1]} = \theta^{[t]}+\hat \lambda{\bf s}
$$
In order to solve the not compute gradient problem, there is the solution that:
$$
\frac{\partial f}{\partial\theta_j} \approx \frac{f[\theta+a{\bf e}_j]-f[\theta]}{a}
$$
where $\bf e_j$ is the unit vector in the $j^{th}$ direction.
\subsubsection*{Newton's method}
\end{document}




















